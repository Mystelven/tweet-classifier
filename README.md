# Tweet Classifier

Learning-ception: I wanted to learn how to use Machine Learning and Natural Language Processing in practice.

This program can classify tweets on a topic (passed in argument) on 3 levels 'positive', 'negative' and 'neutral'.

# How to use the program

You must add a file `twitter4j.properties` at the root of the project.
In this file you must specify some informations from your app credential (https://apps.twitter.com/)
The file is looking as follow:

    debug                   = false
    oauth.consumerKey       = ********
    oauth.consumerSecret    = ********
    oauth.accessToken       = ********
    oauth.accessTokenSecret = ********

Then from the command `mvn compile install exec:java -Dexec.mainClass=Main` you can execute the program.

And with the command `mvn compile install exec:java -Dexec.mainClass=Main -Dexec.args="Trump"`
you can know what Twitter is globally thinking about the new PotUS ;)

At the date of 05/06/2017, here is what Twitter was thinking about Trump:

        [0.987, 0.944, 0.89, 0.845, 0.845, 0.836, 0.836, 0.836, 0.836, 0.836, 0.836, 0.836, 0.836, 0.836, 0.777, 0.774, 0.761, 0.743, 0.741, 0.741, 0.741, 0.741, 0.741, 0.741, 0.734, 0.718, 0.717, 0.717, 0.714, 0.706, 0.706, 0.706, 0.706, 0.693, 0.693, 0.693, 0.688, 0.688, 0.686, 0.684, 0.684, 0.684, 0.684, 0.684, 0.684, 0.684, 0.684, 0.678, 0.66, 0.659, 0.653, 0.642, 0.639, 0.638, 0.635, 0.635, 0.635, 0.635, 0.635, 0.635, 0.635, 0.628, 0.62, 0.616, 0.603, 0.602, 0.598, 0.596, 0.59, 0.589, 0.589, 0.587, 0.584, 0.584, 0.581, 0.58, 0.573, 0.573, 0.569, 0.569, 0.569, 0.566, 0.563, 0.563, 0.562, 0.553, 0.548, 0.548, 0.547, 0.546, 0.543, 0.543, 0.531, 0.529, 0.529, 0.525, 0.525, 0.524, 0.523, 0.521, 0.519, 0.519, 0.517, 0.509, 0.505, 0.504, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.494, 0.494, 0.493, 0.493, 0.493, 0.493, 0.493, 0.493, 0.49, 0.487, 0.486, 0.482, 0.479, 0.474, 0.472, 0.47, 0.468, 0.468, 0.468, 0.468, 0.468, 0.467, 0.467, 0.467, 0.466, 0.466, 0.462, 0.461, 0.46, 0.458, 0.458, 0.449, 0.449, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.442, 0.44, 0.44, 0.44, 0.44, 0.439, 0.439, 0.438, 0.438, 0.43, 0.425, 0.424, 0.423, 0.422, 0.417, 0.416, 0.414, 0.414, 0.414, 0.414, 0.413, 0.412, 0.411, 0.41, 0.408, 0.405, 0.405, 0.405, 0.402, 0.402, 0.402, 0.401, 0.401, 0.399, 0.399, 0.399, 0.399, 0.399, 0.396, 0.396, 0.396, 0.396, 0.394, 0.393, 0.393, 0.393, 0.389, 0.388, 0.384, 0.384, 0.379, 0.373, 0.372, 0.371, 0.36, 0.354, 0.35, -0.351, -0.356, -0.356, -0.358, -0.358, -0.36, -0.365, -0.371, -0.376, -0.376, -0.376, -0.376, -0.376, -0.378, -0.379, -0.379, -0.382, -0.383, -0.386, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.389, -0.39, -0.39, -0.39, -0.392, -0.393, -0.394, -0.396, -0.397, -0.397, -0.397, -0.397, -0.397, -0.397, -0.397, -0.397, -0.397, -0.397, -0.397, -0.397, -0.397, -0.397, -0.397, -0.397, -0.397, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.399, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.401, -0.402, -0.402, -0.403, -0.404, -0.405, -0.405, -0.407, -0.408, -0.408, -0.408, -0.409, -0.413, -0.413, -0.413, -0.414, -0.417, -0.419, -0.419, -0.419, -0.422, -0.423, -0.424, -0.425, -0.426, -0.426, -0.427, -0.428, -0.428, -0.429, -0.429, -0.429, -0.433, -0.433, -0.433, -0.434, -0.434, -0.434, -0.435, -0.437, -0.438, -0.439, -0.439, -0.44, -0.44, -0.441, -0.442, -0.442, -0.442, -0.442, -0.443, -0.443, -0.443, -0.443, -0.443, -0.443, -0.444, -0.444, -0.444, -0.445, -0.445, -0.447, -0.447, -0.447, -0.447, -0.447, -0.449, -0.449, -0.449, -0.449, -0.449, -0.451, -0.451, -0.451, -0.452, -0.452, -0.452, -0.452, -0.452, -0.452, -0.452, -0.453, -0.454, -0.456, -0.456, -0.456, -0.457, -0.457, -0.457, -0.457, -0.457, -0.457, -0.457, -0.457, -0.457, -0.457, -0.46, -0.462, -0.462, -0.462, -0.463, -0.463, -0.463, -0.463, -0.463, -0.463, -0.463, -0.463, -0.464, -0.464, -0.466, -0.466, -0.467, -0.467, -0.468, -0.468, -0.468, -0.468, -0.469, -0.469, -0.469, -0.47, -0.471, -0.471, -0.471, -0.471, -0.471, -0.471, -0.471, -0.471, -0.471, -0.471, -0.471, -0.471, -0.471, -0.471, -0.472, -0.474, -0.477, -0.477, -0.484, -0.485, -0.486, -0.487, -0.488, -0.488, -0.489, -0.492, -0.492, -0.492, -0.493, -0.493, -0.494, -0.494, -0.496, -0.496, -0.497, -0.498, -0.499, -0.499, -0.499, -0.5, -0.503, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.505, -0.506, -0.507, -0.507, -0.509, -0.511, -0.512, -0.515, -0.517, -0.517, -0.518, -0.52, -0.521, -0.521, -0.521, -0.521, -0.523, -0.523, -0.524, -0.524, -0.524, -0.528, -0.531, -0.532, -0.534, -0.534, -0.534, -0.534, -0.534, -0.534, -0.534, -0.534, -0.534, -0.534, -0.534, -0.534, -0.535, -0.539, -0.54, -0.541, -0.541, -0.543, -0.544, -0.544, -0.544, -0.544, -0.544, -0.544, -0.544, -0.544, -0.551, -0.554, -0.555, -0.555, -0.557, -0.559, -0.559, -0.561, -0.564, -0.566, -0.567, -0.568, -0.568, -0.568, -0.568, -0.568, -0.569, -0.569, -0.569, -0.57, -0.571, -0.572, -0.573, -0.574, -0.58, -0.582, -0.582, -0.582, -0.584, -0.585, -0.585, -0.585, -0.586, -0.586, -0.587, -0.588, -0.591, -0.596, -0.597, -0.601, -0.604, -0.604, -0.608, -0.611, -0.613, -0.62, -0.622, -0.623, -0.624, -0.624, -0.625, -0.627, -0.628, -0.629, -0.633, -0.633, -0.635, -0.638, -0.638, -0.642, -0.648, -0.648, -0.648, -0.651, -0.652, -0.652, -0.652, -0.652, -0.652, -0.652, -0.653, -0.653, -0.654, -0.655, -0.655, -0.659, -0.661, -0.667, -0.672, -0.674, -0.677, -0.677, -0.681, -0.684, -0.685, -0.693, -0.693, -0.694, -0.701, -0.701, -0.701, -0.701, -0.704, -0.706, -0.71, -0.712, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.713, -0.714, -0.717, -0.717, -0.718, -0.719, -0.72, -0.725, -0.727, -0.727, -0.752, -0.76, -0.76, -0.76, -0.76, -0.76, -0.76, -0.76, -0.76, -0.76, -0.76, -0.76, -0.76, -0.76, -0.76, -0.76, -0.76, -0.762, -0.763, -0.769, -0.772, -0.773, -0.774, -0.791, -0.799, -0.805, -0.838, -0.842, -0.844, -0.876, -0.876, -0.884, -0.915, -0.923, -0.974]

        The strongest negative  :  -0.974
        The strongest positive  :   0.987
        The median value        :  -0.428
        The average value       :  -0.190

        Which can somehow be interpreted as 'globally, Twitter is against, but when people are for him, they are strongly for him'

The already trained classifier is : https://app.monkeylearn.com/main/classifiers/cl_qkjxv9Ly/tab/tree-sandbox/

# Credits

- [Twitter4J](http://twitter4j.org/en/index.html)
- [Gson](https://github.com/google/gson)
- [Twitter Apps](https://apps.twitter.com)
- [MonkeyLearn](https://monkeylearn.com/)
- [Maven](https://maven.apache.org/)


# Contact

Feel free to contact me on valentin.montmirail@gmail.com for more information

and/or to fork this program to add more features ;).

